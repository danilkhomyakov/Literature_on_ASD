# Metacognitive monitoring and control processes in children with autism spectrum disorder: Diminished judgement of confidence accuracy 

Catherine Grainger ${ }^{\mathrm{a}, *}$, David M. Williams ${ }^{\mathrm{a}}$, Sophie E. Lind ${ }^{\mathrm{b}}$<br>${ }^{a}$ School of Psychology, Keynes College, University of Kent, Canterbury CT2 7NP, United Kingdom<br>${ }^{\mathrm{b}}$ Autism Research Group, Department of Psychology, Rhind Building, City University London, Northampton Square, London, EC1V 0HB, United Kingdom

## A R T I C L E I N F O

Article history:
Received 15 October 2015
Revised 17 February 2016
Accepted 4 March 2016

Keywords:
Autism spectrum disorder (ASD)
Metacognition
Metamemory
Self-awareness
Judgments of confidence (JOC)

A B S T R A C T

Metacognition consists of monitoring processes (the ability to accurately represent one's own mental states) and control processes (the ability to control one's cognitive processes effectively). Both processes play vital roles in self-regulated learning. However, currently it is unclear whether these processes are impaired in individuals with autism spectrum disorders (ASDs). This study aimed to assess metacognition in thirty-two children with ASD, and 30 IQ-/age-matched neurotypical children, using a judgment of confidence task. It was found that children with ASD showed diminished accuracy in their judgments of confidence, indicating metacognitive monitoring impairments in ASD. Children with ASD also used monitoring to influence control processes significantly less than neurotypical children, despite little evidence of impairments in overall control ability.
(c) 2016 Elsevier Inc. All rights reserved.

## 1. Introduction

Metacognition refers to an individual's beliefs and knowledge about cognition (often referred to as metacognitive knowledge), as well as an individual's ability to monitor and control their own cognitive processes (often referred to as metacognitive skill). According to the standard definition, metacognition involves forming meta-representations (second order representations) of one's own mental states. Put more simply, metacognition involves "thinking about thinking" (Flavell, 2000). It is widely thought that accurate metacognitive skill is crucial for effective self-regulation of cognition and behaviour (e.g., Nelson \& Narens, 1990). By accurately monitoring one's own mental states, one can gain a degree of control over those mental states and, by extension, control the behaviour elicited by those states (e.g., Perner, 1991). For example, whilst revising for an exam, if an individual is able to accurately assess what information they know/do not know, they can employ more effective revision techniques, thus ultimately improving their memory for the exam topic.

Whilst metacognition clearly plays an important role in self-regulation, meta-representation is also thought to play an important role in enhancing other aspects of cognition. For example, Perner (e.g., Perner, 2000) has suggested that the ability to distinguish one's thoughts from reality is a pre-requisite for episodic remembering (but see Williams, 2010). Additionally, there is evidence to suggest that such self-related processing is involved in imagining one's own future, and is involved in the processes of episodic future thinking (see for e.g. Wheeler, Stuss, \& Tulving, 1997). In this latter respect, metacognition may

[^0]
[^0]:    * Corresponding author.

    E-mail addresses: cg341@kent.ac.uk (C. Grainger), D.M.Williams@kent.ac.uk (D.M. Williams), Sophie.Lind.2@city.ac.uk (S.E. Lind).

play a role not only in controlling current behaviour, but also future behaviour by facilitating efficient planning for one's future.

However, the area of cognition most frequently discussed in relation to metacognition is mindreading (i.e., the ability to represent the mental states of other people; also known as "theory of mind"). There is a substantial debate that spans developmental, cognitive, and comparative psychology, as well as philosophy and cognitive science, about the relation between mindreading and metacognition. Whereas some argue that mindreading and metacognition rely on the same underlying (metarepresentational) mechanism/processing resources (e.g., Carruthers, 2009; Gopnik, 1993), henceforth termed "one mechanism" theories, others argue either that (a) metacognition and mindreading are unrelated (e.g., Nichols \& Stich, 2003) or that (b) mindreading is dependent upon metacognition, but not vice versa (e.g., Goldman, 2006).

# 2. Metacognition, mindreading and autism spectrum disorder 

One developmental disorder that is relevant to the study of all of the above issues is autism spectrum disorder (ASD). ASD is diagnosed on the basis of behavioural deficits in social-communication, and fixated interests and repetitive behaviours (American Psychiatric Association, 2013). There is substantial evidence that, at the cognitive level, individuals with ASD manifest impairments in episodic memory (see e.g., Crane \& Goddard, 2008; Lind \& Bowler, 2010; Lind, Williams, Bowler, \& Peel, 2014; Losh \& Capps, 2003), episodic future thinking (see Crane, Lind, \& Bowler, 2013; Lind \& Bowler, 2010; Lind, Bowler, \& Raber, 2014; Terrett et al., 2013), cognitive flexibility (e.g., Ambery, Russell, Perry, Morris, \& Murphy, 2006; Ozonoff, Pennington, \& Rogers, 1991), and mindreading (see Yirmiya, Erel, Shaked, \& Solomonica-Levi, 1998). However, despite the wealth of studies being conducted on these cognitive abilities (all of which are thought to be related to metacognition) in ASD, only a handful of studies have ever explored metacognitive skill in this disorder. This is surprising, given that the study of metacognition in ASD could have significant implications for both theory and practice. For example, there is almost universal agreement that mindreading ability is diminished in ASD. Therefore, if metacognitive monitoring was found to be quantitatively and qualitatively typical among people with this disorder, then this would rule out a one mechanism view of the relation between metacognition and mindreading (e.g., Carruthers, 2009).

To date, much of the evidence regarding metacognitive monitoring in ASD has come from studies that have employed "self-versions" of classic mindreading tasks, in which participants are required to explain their own behaviour on the basis of their mental states (see Williams, 2010, for a review). For example, in the classic unexpected contents false belief task, participants are asked what they believe is contained inside a familiar container (e.g., a Smarties tube). Having expressed their belief that the container contains its usual contents (e.g., Smarties) participants are then shown that it contains something unexpected (e.g., pencils). Participants are then often asked a critical metacognitive test question regarding what they thought was inside the box before they were shown. A correct answer to this question would, of course, be Smarties. However, in keeping with the suggestion that metacognition may be impaired in ASD, several studies have observed impairments on self-versions of such tasks, in children with ASD (see e.g., Baron-Cohen, 1992; Fisher, Happé, \& Dunn, 2005; Russell, Hill, \& Franco, 2001; Williams \& Happé, 2009).

However, whilst such findings may be interpreted as evidence of impaired metacognitive abilities in ASD, some have argued that there is a critical limitation with these types of studies that prevents definitive conclusions being drawn (see Carruthers, 2009; Nichols \& Stich, 2003). Namely, it has been argued that such tasks usually require participants to represent their prior mental states, rather than their current mental states. In the Smarties unexpected contents task, for example, participants believe that the box contains Smarties before they look inside (at time 1). Then the unexpected contents (pencils) are revealed to the participant, which results (at time 2) in a belief that the box contains pencils. Participants are then asked the critical test questions regarding what they believed was inside before they looked inside the box (i.e., what they believed at time 1). But the fact that participants no longer believe (at time 2) what they believed in the past (at time 1), the test question appears to be asking them to report on a prior belief, rather than a current belief. Yet metacognitive monitoring is, by definition, the ability to report on one's current, online mental states. Reporting one's prior, out-of-date beliefs requires reconstruction of one's earlier belief in memory. As such, many do not consider self-versions of standard ToM tasks to be tests of metacognitive monitoring (see e.g., Nichols \& Stich, 2003). As such, the results from the above studies do not necessarily show that metacognition is impaired in ASD.

However, recently, studies of metacognition in ASD have begun to employ tasks that are considered more standard measures of metacognitive skill by researchers in that field and which, it is more widely agreed, require individuals to monitor their own current mental states. One classic paradigm that has been widely used to measure metacognitive ability in neurotypical individuals is a judgment of confidence (JOC) task. Studies assessing judgments of confidence typically involve participants answering questions about recently-studied material or stored semantic knowledge, and then reporting their confidence in the answers they provided. Importantly, is widely accepted that on such tasks participants monitor their current feelings of confidence in their answers. As such, this sort of paradigm overcomes the issue posed by self-versions of ToM tasks. If an individual's metacognitive monitoring ability is good, then their confidence judgements should accurately discriminate between correct and incorrect answers. Additionally, in some JOC paradigms participants are subsequently given the opportunity to exclude some of their answers, such that those answers will not contribute to the participant's final "score". This aspect of self-monitoring relies on metacognitive control (an individual's ability to regulate their cognition)

and is particularly important given that confidence judgements are often used by individuals to regulate decision making and subsequent behavioural choice (see Koriat \& Goldsmith, 1996).

Potential impairments in an individual's ability to make accurate judgements of confidence can have several negative consequences. Indeed, in many professions, an inability to make accurate judgements can have severe consequences. As Hacker and colleagues explain, "such dire consequences are exemplified by a physician who is unrealistically confident in her diagnoses, a lawyer who may be unduly optimistic when predicting the verdicts of his court cases, or an airline pilot who overestimates her ability to handle challenging weather conditions" (Hacker, Bol, Horgan, \& Rakow, 2000). From an educational perspective JOC accuracy is also critical. For example, under-confident students may waste time studying information they have already learnt sufficiently, whereas overconfident students may fail to study information sufficiently, instead falsely believing they have already learnt it (see Hacker et al., 2000). Thus, as well as having theoretical implications, the study of JOC accuracy in individuals with ASD also has important translational implications.

To date, only three studies have assessed JOC accuracy among individuals with ASD and these studies report mixed results (Sawyer, Williamson, \& Young, 2014; Wilkinson, Best, Minshew, \& Strauss, 2010; Wojcik, Allen, Brown, \& Souchay, 2011). In Wilkinson et al. (2010; Exp. 1), children with ASD, as well as age- and IQ-matched neurotypical comparison participants, were tested for their ability to recognise (via an old/new recognition test) recently-presented faces. After each response during the recognition test phase, participants made a confidence judgement about their answer, reporting whether they were 'certain', 'somewhat certain', or 'guessing'. Wilkinson et al. (2010) found that the confidence judgments made by children with ASD were significantly less accurate than those made by neurotypical children, implying diminished metacognitive monitoring in ASD. However, the same procedure among adults (Exp. 2) revealed no significant betweengroup differences in JOC accuracy, leading Wilkinson et al. to conclude that metacognition was not diminished among adults with ASD. Despite this conclusion, it is notable that, although not statistically significant, more than a quarter of the answers adults with ASD reported they were certain of were, in fact, incorrect. Whilst neurotypical adults got $85 \%$ of the answers they reported they were certain of correct, adults with ASD only got $72 \%$ of their 'certain' answers correct, and this difference was moderate in size (Cohen's $d=0.53$ ). This suggests at least a subtle diminution of meta-monitoring ability even in adults with ASD. However, one difficulty with drawing firm conclusions from Wilkinson et al.'s (2010) study is that, in both the adult and child experiments, memory awareness was assessed during a facial recognition task. Given that research suggests that individuals with ASD show impairments in face processing (e.g., Hauck, Fein, Maltby, Waterhouse, \& Feinstein, 1998; Williams, Goldstein, \& Minshew, 2005), there could be concern that impairments at the cognitive level (or 'object-level'), rather than metacognitive level, of this task could have confounded individuals' confidence judgements.

In a second study, Wojcik et al. (2011) asked children to make confidence judgements concerning whether they had correctly performed a series of recently-observed actions. In contrast to Wilkinson et al. (2010), this study reported no significant between-group differences in JOC accuracy, implying that meta-monitoring is undiminished in ASD.

It is important to note that neither the study by Wilkinson et al. (2010) nor the study by Wojcik et al. (2011) assessed metacognitive control in ASD. To our knowledge, the only study ever to have assessed monitoring and control in ASD was by Sawyer et al. (2014), who employed a JOC task that assessed both abilities in adults with ASD. In this study, participants were asked to complete an emotion recognition task involving facial stimuli. Participants were instructed that the aim of the study was to submit as many correct responses as possible. For each emotion recognition judgement, participants rated how confident they were that they had selected the correct response. Participants were then given the opportunity to submit each answer towards their total score (and gain a point for each correct answer), or discard the answer (and avoid losing a point for getting an answer wrong). This provided a measure of metacognitive control. In a second experiment, the same procedure was used but participants' judgements concerned their answers to general knowledge questions, rather than emotion recognition.

In both experiments, Sawyer et al. reported no significant between-group differences in JOC accuracy, implying undiminished meta-monitoring ability in ASD. In terms of metacognitive control, Sawyer et al. (2014) also found no between-group differences on their key index ( $d^{1}$ ), implying undiminished metacognitive control in ASD. That being said, Sawyer et al. performed additional post-hoc tests, which indicated that a significantly higher proportion of ASD participants ( $n=12$ ) than neurotypical participants decided not to withhold any answers on the emotion recognition task. This could imply that these 12 ASD participants were not showing any metacognitive control at all. Alternatively, it could reflect a complete failure to understand the task demands among these participants. On this basis, the extent to which metacognitive control is diminished in ASD is still unclear. What is particularly notable is that these 12 ASD participants did appear to show diminished metacognitive monitoring ability relative to the neurotypical control group, the difference in the dependent variable (gamma score) being associated with a Cohen's $d$ value of 0.62 according to our calculations.

Although the study by Sawyer et al. (2014) was well conducted in many respects, there are additional issues that might suggest caution should be taken when interpreting the results. Importantly, participant groups were not matched for age or performance IQ ${ }^{1}$. Matching for baseline variables is essential in such studies, because differences between groups in this respect can potentially explain entirely between-group differences in experimental task performance (see Mervis \& Klein-Tasman, 2004). Indeed, Dr. Sawyer kindly provided us with additional unreported data about this (A. Sawyer, personal communication,

[^0]
[^0]:    ${ }^{1}$ Note that Wojcik et al. (2011) do not report whether groups were matched for PIQ and VIQ (reporting overall full-scale IQ), and thus these methodological concerns may also apply to Wojcik et al. with respect to group matching.

August 22nd, 2014). In Sawyer et al., age was significantly negatively associated with $d^{\prime}$ among participants with ASD, $r=-.37$, $p=.04$. This presents a problem when interpreting the results of the study, because the ASD and TD groups in this paper differed on a co-variate of metacognitive task performance. Group differences in age (the ASD group were significantly older than the TD group) could have potentially confounded performance on the task employed and could well explain the trend towards group differences in metacognitive control in this study. Note that whilst it might seem surprising that age is negatively associated with metacognitive control ability (i.e., the older participants were the poorer their metacognitive control) in Sawyer et al., it is not the first study to find such an association (see e.g., Butterfield, Nelson, \& Peck, 1988; Schneider, Visé, Lockl, \& Nelson, 2000; but see Krebs \& Roebers, 2010; Weil et al., 2013). Whatever the explanation is for this negative association, the failure to equate participant groups on this covariate in Sawyer et al. represents a significant limitation of the study.

# 3. The current study 

The central aim of this study was to extend the current findings concerning metacognition in ASD, by examining both monitoring and control accuracy in children with ASD. To examine this the study employed a JOC task, during which children were asked a series of questions about recently-studied material, and were then asked to judge how certain they were that the answers they had provided were correct (providing a measure of metacognitive monitoring accuracy). Additionally, children were told that for each correct answer they submitted they would receive a point, but for each incorrect answer they would lose a point. At the end of the task children were given the opportunity to remove any of the answers they had previously provided (providing a measure of metacognitive control accuracy). Our main prediction was that participants with ASD would demonstrate impairments in metacognitive monitoring ability. This was predicted on the basis of our theoretical inclinations, as well as our interpretation/critical analysis of the few previous studies of this ability in ASD. Our prediction about group differences in metacognitive control ability was less straightforward. The only study ever (by Sawyer et al., 2014) to explore this ability in ASD reported a trend towards a group difference in this ability, but there are arguably difficulties with Sawyer et al.'s findings that prevent definitive conclusions from being drawn (see above). Our prediction for this aspect of the study was non-directional, therefore. The issue of whether metacognitive monitoring and/or control are diminished in ASD is separate from the issue of whether monitoring is used for the purpose of control by people with ASD. For example, it is possible that monitoring ability is intact in ASD, but not used appropriately for the purpose of metacognitive control. Alternatively, even if monitoring ability is diminished in ASD, residual monitoring ability might influence control processes to the same extent among individuals with ASD as among neurotypical individuals. Given these possibilities and given the fact that no previous study has explored the extent to which monitoring influences control in ASD, we also made a non-directional prediction with respect to this aspect of the study.

## 4. Method

### 4.1. Participants

Ethical approval for this study was obtained from the University of Kent School of Psychology Research Ethics Committee. Thirty-two children with ASD and 30 neurotypical children took part in this study, after their parents had given written, informed consent. Participants in the ASD group had formal diagnoses of autistic disorder or Asperger's disorder, according to established criteria (American Psychiatric Association, 2000; World Heath Organisation, 1993). To assess severity of ASD features, parents of participants with ASD completed the Social Responsiveness Scale (SRS; Constantino et al., 2003). In all but one case, participants with ASD scored above the defined cut-off for ASD on the SRS (total score $>60$; Constantino et al., 2003). The remaining participant scored 55 on the SRS, which is just below the conventional ASD cut-off of 60.

Parents of the neurotypical children also completed the SRS. All but four participants in the neurotypical group scored below the defined cut-off for ASD. The remaining participants' SRS scores ranged between 60 and 73. To ensure that including these participants in our sample did not affect the results of the study all analyses in the paper were re-run, excluding all five participants who scored outside the expected range on the SRS. After removing these participants from analyses, none of the results (nor study conclusions) changed substantively (i.e., no $p$ value changed from significant to non-significant or vice versa, and no effect size changed category - small, moderate, large).

The participant groups were closely equated for verbal and non-verbal ability (see Table 1 for participant characteristics), using the Wechsler Abbreviated Scale of Intelligence (WASI; Wechsler, 1999). Both groups were also equated closely for chronological age.

### 4.2. Materials and procedures

### 4.2.1. Judgement of confidence task

This task was designed to assess the accuracy of children's metamemory monitoring and control processes, and was based heavily on a design employed among typically developing children (e.g., Krebs \& Roebers, 2010; Roebers, Schmid, \& Roderer, 2009). The task consisted of a study phase, a test phase, a JOC phase (during which confidence in the accuracy of recall was

Table 1
Participant characteristics (means, standard deviations and inferential statistics).

|  | Group |  | $t$ | $p$ | Cohen's $d$ | 95\% Confidence intervals (for mean difference score) |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  | ASD $(n=32)$ | Neurotypical $(n=30)$ |  |  |  | Lower CI | Upper CI |
| Age (years) | $13.59(1.36)$ | $13.27(1.06)$ | 1.01 | .315 | 0.26 | $-0.31$ | 0.94 |
| VIQ | $101.28(16.69)$ | $103.87(14.92)$ | 0.64 | .524 | 0.16 | $-10.65$ | 5.48 |
| PIQ | $100.72(13.39)$ | $105.67(14.32)$ | 1.41 | .165 | 0.36 | $-11.99$ | 2.09 |
| FSIQ | $101.19(14.85)$ | $105.53(15.27)$ | 1.14 | .261 | 0.29 | $-12.00$ | 3.31 |
| SRS total score | $84.16(8.79)$ | $45.67(10.50)$ | 15.69 | $<.001$ | 3.98 | 33.58 | 43.40 |

SRS: Social Responsiveness Scale (Constantino et al., 2003); VIQ = verbal IQ; PIQ = performance IQ; FSIQ = full scale IQ.
assessed), and a "metacognitive control phase" (during which the accuracy of metacognitive control processes were assessed). In total, the task took approximately twenty minutes to complete.
4.2.1.1. Study phase. Participants were shown a short ( 4 min ) video, presented to them on a laptop computer. This video was downloaded from a website of educational videos, suitable for 11- to 16-year-olds. The experimenter explained to each participant that the video they were about to watch was about kangaroos and about how kangaroos survive in the Australian Outback. Participants were told to pay full attention to the video, because afterwards they would be asked some questions about the information presented in it.
4.2.1.2. Test phase. After watching the video, participants were given a short worksheet, which consisted of 16 test questions, for which the answer had been explicitly presented in the video, during the study phase. Additionally, the sheet included eight questions to ensure that participants understood the rating scale for their subsequent confidence judgements. Four "easy" questions were designed so that it was almost certain participants would know the answers to them (e.g., how many eyes do kangaroos have?) and four "impossible" questions were included so that it would be very unlikely children would know the correct answers (e.g., what is the Latin species name for kangaroo?). If participants were not able to make the clear distinction in their judgements of confidence between answers to easy and impossible questions, then one could not be sure they understood the nature of the task at all.

For the test phase, the edge of the worksheet containing the JOC scale was folded over, so that it was not visible. Once presented with the worksheet, participants were given a pen, and were asked to write down an answer for each question. It was explained to participants that it was important they provided an answer for every question, and that if they did not know the correct answer they should take a guess. In cases where participants were reluctant to answer a particular question, the experimenter reminded them that it was important they answer each question, and that they should make a guess if they could not remember the correct answer.
4.2.1.3. JOC phase. After the experimenter checked that participants had provided an answer for all the questions on the worksheet, they turned over the section of the worksheet that had previously been folded over, making the JOC scale visible. The JOC scale remained visible for the rest of the task (both the 'JOC phase' and 'Metacognitive Control Phase'). It was explained to participants that "I would now like to know how confident you are that each of the answers you wrote down is correct". Participants were asked to judge their confidence for each answer on a 7-point likert scale, ranging from 1 to 7 (extremely unsure to extremely sure). The experimenter fully explained the confidence scale to participants, explaining that higher numbers on the scale indicated a higher certainty that the answer provided was correct. Participants gave confidence judgments for each answer on a scale next to that answer on the worksheet. The experimenter made sure that each participant provided a JOC for each answer.
4.2.1.4. Metacognitive control phase. Finally, participants were told that at a later point the experimenter would mark each of the answers on the worksheet. It was explained to participants that for each correct answer they had given they would get one point, but for each incorrect answer one point would be taken away from them. Participants were then given a different coloured pen and told that they had the opportunity to improve their performance on the task. Participants were told that they were now able to cross out any of their answers. If they crossed out an answer they would neither get a point for this answer if that answer was correct, nor lose a point if the answer was incorrect. Participants were told that they could cross out as many or as few answers as they liked. Participants could see both their answers and their JOC ratings for each answer during this phase of the task.

# 4.3. Scoring 

### 4.3.1. Judgement of confidence task

4.3.1.1. Object-level test performance. A measure of participants' object-level memory performance was calculated on the JOC task. Participants recall ability was calculated as the proportion of answers they correctly remembered during the recall stage of the task.
4.3.1.2. JOC rating scale use. The average confidence judgements given by participants for answers to the 'easy' and 'impossible' questions was calculated.
4.3.1.3. Metacognitive monitoring accuracy. Firstly, the average confidence judgement for correct answers and the average confidence judgement for incorrect answers were calculated. The better participants' monitoring accuracy, the larger the difference should be between their confidence ratings for correct answers compared to incorrect answers. Thus, this difference score provided a basic measure of metacognitive accuracy.

Secondly, gamma scores (Goodman \& Kruskal, 1954) were calculated to provide an index of JOC accuracy for each participant. This type of analysis is recommended by Nelson (1984), and Nelson, Narens, and Dunlosky (2004), and is commonly used to analyse monitoring accuracy on JOC tasks (e.g., Roebers et al., 2009; Sawyer et al., 2014). Gamma correlations range between +1 and -1 ; a score of 0 indicates chance-level accuracy, in which confidence judgements are not associated in any way with whether an answer is correct or not. A large positive gamma value indicates a high correspondence between confidence in the correctness of one's answers and the actual correctness of one's answers. A large negative value indicates that confidence judgments were inversely related to their recall performance (i.e., high confidence in incorrect answers and low confidence in correct answers). Gamma scores were calculated individually for each participant, based on their answers to the 16 experimental questions and their corresponding confidence judgments.
4.3.1.4. Metacognitive control accuracy. Control effectiveness was calculated using the measure of d-prime ( $d^{\prime}$ ). This measure is how control accuracy was assessed in Sawyer et al. (2014), and is calculated using participants' 'hit-rate' $(H)$ and 'falsealarm rate' (FA). Hit rate was calculated as the number of answers participants removed that were incorrect (and thus correctly removed) plus the number of answers participants kept that were correct (and thus correctly kept), divided by the total number of answers. False alarm rate was calculated as the number of answers participants removed that were in fact correct (and thus incorrectly removed) divided by the total number of correct answers participants provided. $d^{\prime}$ was calculated using the following formula:

$$
d^{\prime}=Z(H)-Z(F A)
$$

A $d^{\prime}$ score of 0 indicates no difference between hit rate and false alarm rate, demonstrating ineffective metacognitive control strategies on the task. In contrast, the higher the $d^{\prime}$ value, the greater the tendency to remove incorrect answers and keep correct answers, thus the greater the effectiveness of the control strategy on the task.
4.3.1.5. The effect of monitoring processes on control accuracy. The average confidence judgment participants gave for the answers they removed was calculated, as was the average confidence judgement participants gave for the answers they kept. These scores were calculated to provide a measure of how participants' monitoring judgements influenced their control performance (see e.g., Krebs \& Roebers, 2010). The more participants' control processes were influenced by their JOC ratings accuracy, the larger the difference should be between their confidence ratings for kept answers compared to answers they removed. Thus, this difference score provided a basic measure of metacognitive control accuracy. Two participants (1 ASD, 1 neurotypical) chose not to remove any of their answers and thus difference scores for these participants could not be calculated.

## 5. Results

### 5.1. Judgment of confidence task

Table 2 shows descriptive statistics and the results of independent-samples $t$-tests for all aspects of the experimental JOC task. Given the predicted group differences in meta-level monitoring performance on the JOC task, all $p$ values associated with group differences on this aspect of the task are reported one-tailed.

Before reporting the main results, it is important to ensure that participants were able to use the JOC rating scale appropriately. Thus, an initial analysis of group differences in the average JOC rating for the four 'easy' and four 'impossible' questions was conducted (please see Table 2 for the descriptive statistics). A mixed-model ANOVA revealed a significant main effect of Question-type, reflecting that JOC ratings were significantly higher for 'easy' questions than for 'impossible' questions, $F(1,60)=678.21, p<.001, \eta_{p}^{2}=0.91$. Crucially, neither the main effect of Group nor the Group $\times$ Question-type interaction were significant, $F s<0.57, p s>.45$. This suggests that both groups were able to use the rating scale appropriately.

Table 2
Means (SDs) and inferential statistics for group differences in performance on the JOC task.

| Experimental measure | Dependent variable | Group |  | $t$ | $p$ | Cohen's <br> d | 95\% Confidence <br> intervals (for <br> mean difference <br> score) |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  |  | ASD | Neurotypical |  |  |  | Lower <br> CI | Upper <br> CI |
| Object-level performance | Proportion of answers recalled | .58 <br> (.18) | $.60(.12)$ | 0.57 | .572 | 0.13 | $-0.10$ | $-0.06$ |
| Meta-level performance | Mean JOC for 'easy' questions | $\begin{aligned} & 6.06 \\ & (1.10) \end{aligned}$ | $6.16(0.81)$ | 0.39 | .699 | 0.10 | $-0.59$ | $-0.40$ |
|  | Mean JOC for 'impossible' questions | $\begin{aligned} & 2.47 \\ & (0.99) \end{aligned}$ | $2.35(0.81)$ | 0.52 | .608 | 0.13 | $-0.34$ | $-0.58$ |
|  | Mean difference between JOC for correct and incorrect answers | $\begin{aligned} & 2.63 \\ & (1.08) \end{aligned}$ | $3.46(0.76)$ | 3.47 | .001 ${ }^{\text {a }}$ | 0.89 | $-1.32$ | $-0.35$ |
|  | Gamma score* | $\begin{aligned} & .84 \\ & (.16) \end{aligned}$ | $.90(.13)$ | 1.75 | .043 ${ }^{\text {a }}$ | 0.41 | $-0.14$ | $-0.01$ |
| Control performance | d-prime** | $\begin{aligned} & 4.15 \\ & (1.56) \end{aligned}$ | $4.74(1.54)$ | 1.48 | .145 | 0.38 | $-1.37$ | 0.21 |
| Influence of monitoring over control | Mean difference between JOC for kept and removed answers | $\begin{aligned} & 3.29 \\ & (1.05) \end{aligned}$ | $3.91(0.88)$ | 2.43 | .018 | 0.64 | $-1.12$ | $-1.09$ |

Note: ASD = autism spectrum disorder; JOC = Judgment of confidence.
${ }^{a} p$ values reported one-tailed, because the direction of the effect was predicted a priori.
* Gamma scores index metamemory monitoring accuracy.
** d-prime scores index metamemory control accuracy.

### 5.1.1. Cognitive (object-level) test performance

An independent-samples $t$-test revealed no significant between-group difference in the proportion of answers correctly recalled.

### 5.1.2. Metacognitive monitoring performance

An independent-samples $t$-test revealed significantly lower gamma scores among ASD than neurotypical participants. Likewise, an independent-samples $t$-test showed that the difference between JOC ratings for correct answers and JOC ratings for incorrect answers was significantly smaller among ASD participants than neurotypical participants. Both of these results suggest diminished metacognitive monitoring among participants with ASD.

### 5.1.3. Metacognitive control performance

An independent-samples $t$-test indicated that the between-group difference in $d^{\prime}$ scores was non-significant, $p=.145$, Cohen's $D=0.38$.

### 5.1.4. The influence of monitoring on control processes

An independent-samples $t$-test showed that the difference between JOC ratings for kept answers and JOC ratings for removed answers was significantly smaller among ASD participants than neurotypical participants. This suggests that, when deciding which answers to remove, individuals in the ASD group relied on their JOC ratings less than neurotypical individuals.

## 6. Discussion

As predicted, this study found that participants with ASD showed diminished metacognitive monitoring accuracy, as reflected by significant between-group differences in gamma scores and in the difference score between JOC ratings for correct versus incorrect answers. These results are in keeping with our predictions and the findings of Wilkinson et al. (2010), who also found that confidence judgments made by children with ASD were less accurate than those made by neurotypical children. However, these results are not in keeping with two previous studies, that concluded JOC accuracy was unimpaired in ASD (Sawyer et al., 2014; Wojcik et al., 2011). Importantly, in our study participants with ASD were (a) closely-matched with neurotypical participants in terms of age, VIQ, and PIQ, and (b) unimpaired on the basic object-level of the task.

Although we found clear evidence that metacognitive monitoring is diminished among children with ASD, we found little evidence to support the idea that metacognitive control was similarly diminished; the between-group difference in $d^{\prime}$ did not approach statistical significance. Having said this, it is notable that $d^{\prime}$ was lower among participants with ASD than among neurotypical participants (albeit non-significantly so) and that the effect size associated with the between-group difference

was moderate in size. On the one hand, this might suggest that this ability is subtly diminished in ASD. On the other hand, if such a diminution exists, it is questionable whether it is clinically significant, given the small magnitude of the effect.

Clearer evidence of a meaningful difference between the groups was found with regard to the extent to which metacognitive monitoring influenced metacognitive control. Here, we found that the mean difference between JOC ratings for kept and removed answers was significantly smaller among ASD participants than among comparison participants, indicating that monitoring influenced control to a significantly lesser extent among participants with ASD than among comparison participants. This suggests that the means by which individuals achieved control over their behavioural choices (i.e., which answers to remove) was somewhat different among ASD and comparison groups. One explanation for this is that participants with ASD in the current sample employed a compensatory strategy on the JOC task, allowing them to perform relatively well on the control task despite impaired monitoring ability. For example, during the control stage of the task, participants' decisions to keep or remove an answer could have been determined simply by whether or not they could bring an answer to mind, without additional metacognitive monitoring of their confidence in the accuracy of that answer. Such a "blunt", categorical strategy could well have resulted in reasonable success in the control phase of the experimental task. Whether or not such a strategy would be sufficient for effective metacognitive control in more open-ended, naturalistic situations is an open question. Of course, these explanations are purely speculative, but the idea that atypical resources are recruited by individuals with ASD to succeed on experimental tasks is plausible and arguably deserves investigation in further studies.

From a theoretical perspective, findings of diminished metacognitive monitoring in ASD are most consistent with predictions made by one mechanism accounts of the relation between metacognition and mindreading abilities (Carruthers, 2009; Frith \& Happé, 1999). Such theories predict that both metacognition and mindreading rely on the same underlying mechanisms, and predict that metacognition should be impaired in individuals with ASD (e.g., Carruthers, 2009). In contrast, both simulation theory and the two mechanisms theory directly predict that metacognition is unimpaired in ASD. Of course, the results of this study do rule out two systems views (e.g., Goldman, 2006), because it may be that two systems (one underpinning mindreading and the other underpinning metacognition) exist and that, by chance, both are impaired in ASD.

Research within the typically developing literature suggests that metacognition (confidence judgments in particular) plays an important role in everyday functioning and decision making (see e.g., Yeung \& Summerfield, 2012). In particular, metacognition, plays a vital role in self-regulation, and self-regulated learning (e.g., Hartwig, Was, Isaacson, \& Dunlosky, 2012; Thiede, Anderson, \& Therriault, 2003). Findings that individuals with ASD show impaired metamemory monitoring thus need to be taken into account in educational environments. This is particularly important given that intellectually high-functioning individuals with ASD often show significantly lower academic achievement than would be expected on the basis of their intelligence (see Estes, Rivera, Bryan, Cali, \& Dawson, 2011). Indeed, the educational domains in which people with ASD frequently under-achieve are just those in which learning is known to be fostered by metacognitive training. Such training has been shown to remediate difficulties in reading comprehension (see Brown \& Campione, 1996), writing (e.g., Sitko, 1998) and mathematical reasoning (e.g., Fuchs et al., 2003). In each of these domains, individuals with ASD show statistically significant under-achievement, relative to IQ (see Estes et al., 2011; Jones et al., 2009). It is plausible that diminished metacognitive monitoring contributes to the lower-than-expected levels of academic achievement in ASD in these areas, and this is an issue that should be addressed in future research.

Our results support the notion that metacognitive monitoring is diminished in ASD and that what residual monitoring is available to individuals with ASD is used for the purpose of metacognitive control to a lesser extent than are the monitoring resources available to neurotypical individuals. As such, it is important that future research explores metacognition in ASD further, particularly exploring both metacognitive monitoring and control processes, and how these processes relate to each other. In particular, it will also be important to understand the basis of metacognitive control in ASD with a view to establishing whether the (seemingly alternative) strategies employed by individuals with ASD are as successful in the context of real-world learning as they are in laboratory-based experiments. In our view, assessing metacognitive awareness in populations with cognitive level impairments is a critical matter. Future research should aim to establish a comprehensive account of metacognition in ASD, with the aim of informing intervention efforts designed to remediate cognitive impairments in this disorder.

The authors would like to sincerely thank all of the participants who took part in this study. Without their support, this research would not have been possible. The authors would also like to sincerely thank several schools in Kent for their assistance with the study; The Abbey School, Simon Langton Grammar School for Boys, The Malling School, Grange Park School, Ripplevale School, and Folkestone Academy. We would also like to thank the Kent Autistic Trust (KAT) for assistance with participant recruitment, and Lucy Elias, Merve Kilic and Maddie Musgrove for their assistance with data collection. Finally, we would like to thank Dr. Alyssa Sawyer for being so helpful in response to our queries about her article. Catherine Grainger was funded by an Economic and Social Research Council doctoral studentship, and a University of Kent PhD scholarship.

# References

American Psychiatric Association (2000). Diagnostic and statistical manual of mental disorders (4th edition, text revised) (DSM-IV-TR).Washington, DC: American Psychiatric Association.
American Psychiatric Association (2013). Diagnostic and statistical manual of mental disorders (5th edition).Washington DC: American Psychiatric Association. Baron-Cohen, S. (1992). Out of sight or out of mind? Another look at deception in autism. Journal of Child Psychology and Psychiatry, 33(7), 1141-1155. http:// dx.doi.org/10.1111/j.1469-7610.1992.tb00934.x.

Brown, A. L. \& Campione, J. C. (1996). Psychological theory and the design of innovative learning environments: On procedures, principles, and systems. In L. Schauble \& R. Glaser (Eds.), Innovations in learning: New environments for education. Hillsdale, NJ, England: Lawrence Erlbaum Associates.
Butterfield, E., Nelson, T., \& Peck, V. (1988). Developmental aspects of the feeling of knowing. Developmental Psychology, 24, 654-663.
Carruthers, P. (2009). How we know our own minds: The relationship between mindreading and metacognition. Behavioral and Brain Sciences, 32(2), $121-182$.
Constantino, J. N., Davis, S. A., Todd, R. D., Schindler, M. K., Gross, M. M., Brophy, S. L. ... Reich, W. (2003). Validation of a brief quantitative measure of autistic traits: Comparison of the social responsiveness scale with the autism diagnostic interview-revised. Journal of Autism and Developmental Disorders, 33(4), 427-433. http://dx.doi.org/10.1023/A:1025014929212.
Crane, L., \& Goddard, L. (2008). Episodic and semantic autobiographical memory in adults with autism spectrum disorders. Journal of Autism and Developmental Disorders, 38(3), 498-506. http://dx.doi.org/10.1007/s10803-007-0420-2.
Crane, L., Lind, S. E., \& Bowler, D. M. (2013). Remembering the past and imagining the future in autism spectrum disorder. Memory, 21(2), 157-166. http:// dx.doi.org/10.1080/09658211.2012.712976.

Estes, A., Rivera, V., Bryan, M., Cali, P., \& Dawson, G. (2011). Discrepancies between academic achievement and intellectual ability in higher-functioning school-aged children with autism spectrum disorder. Journal of Autism and Developmental Disorders, 41(8), 1044-1052. http://dx.doi.org/10.1007/ s10803-010-1127-3.
Fisher, N., Happé, F., \& Dunn, J. (2005). The relationship between vocabulary, grammar, and false belief task performance in children with autistic spectrum disorders and children with moderate learning difficulties. Journal of Child Psychology and Psychiatry, 46(4), 409-419. http://dx.doi.org/10.1111/j.14697610.2004.00371.x.

Flavell, J. H. (2000). Development of children's knowledge about the mental world. International Journal of Behavioral Development, 24(1), 15-23. http://dx. doi.org/10.1080/016502500383421.
Frith, U., \& Happé, F. (1999). Theory of mind and self-consciousness: What is it like to be autistic? Mind \& Language, 14(1), 1-22. http://dx.doi.org/10.1111/ 1468-0017.00100.

Fuchs, L. S., Fuchs, D., Prentice, K., Burch, M., Hamlett, C. L., Owen, R., \& Schroeter, K. (2003). Enhancing third-grade students' mathematical problem solving with self-regulated learning strategies. Journal of Educational Psychology, 95(2), 306-315. http://dx.doi.org/10.1037/0022-0663.95.2.306.
Goldman, A. (2006). Simulating minds: The philosophy, psychology and neuroscience of mindreading.Oxford: Oxford University Press.
Goodman, L. A., \& Kruskal, W. H. (1954). Measures of association for cross classifications. Journal of the American Statistical Association, 49(268), 732-764. http://dx.doi.org/10.2307/2281536.
Gopnik, A. (1993). How we know our minds - The illusion of first-person knowledge of intentionality. Behavioral and Brain Sciences, 16(1), 1-14.
Hacker, D. J., Bol, L., Horgan, D. D., \& Rakow, E. A. (2000). Test prediction and performance in a classroom context. Journal of Educational Psychology, 92(1), 160-170. http://dx.doi.org/10.1037//0022-0663.92.1.160.
Hartwig, M. K., Was, C. A., Isaacson, R. M., \& Dunlosky, J. (2012). General knowledge monitoring as a predictor of in-class exam performance. British Journal of Educational Psychology, 82(3), 456-468. http://dx.doi.org/10.1111/j.2044-8279.2011.02038.x.
Hauck, M., Fein, D., Maliby, N., Waterhouse, L., \& Feinstein, C. (1998). Memory for faces in children with autism. Child Neuropsychology, 4(3), 187-198. http:// dx.doi.org/10.1076/cnbs.4.3.187.3174.

Jones, C. R. G., Happé, F., Golden, H., Marsden, A. J. S., Tregay, J., Simonoff, E.,, et al \& Charman, T. (2009). Reading and arithmetic in adolescents with autism spectrum disorders: Peaks and dips in attainment. Neuropsychology, 23(6), 718-728. http://dx.doi.org/10.1037/a0016360.
Koriat, A., \& Goldsmith, M. (1996). Monitoring and control processes in the strategic regulation of memory accuracy. Psychological Review, 103(3), 490-517. http://dx.doi.org/10.1037//0033-295X.103.3.490.
Krebs, S. S., \& Roebers, C. M. (2010). Children's strategic regulation, metacognitive monitoring, and control processes during test taking. British Journal of Educational Psychology, 80(3), 325-340. http://dx.doi.org/10.1348/0007099105485719.
Lind, S. E., \& Bowler, D. M. (2010). Episodic memory and episodic future thinking in adults with autism. Journal of Abnormal Psychology, 119(4), 896-905. http://dx.doi.org/10.1037/a0020631.
Lind, S. E., Bowler, D. M., \& Raber, J. (2014). Spatial navigation, episodic memory, episodic future thinking, and theory of mind in children with autism spectrum disorder: Evidence for impairments in mental simulation? Frontiers in Psychology, 5. http://dx.doi.org/10.3389/fpsyg.2014.01411.
Lind, S. E., Williams, D. M., Bowler, D. M., \& Peel, A. (2014). Episodic memory and episodic future thinking impairments in high-functioning autism spectrum disorder: An underlying difficulty with scene construction or self-projection. Neuropsychology, 28(1), 55-67. http://dx.doi.org/10.1037/neu0000005.
Losh, M., \& Capps, L. (2003). Narrative ability in high-functioning children with autism or Asperger's syndrome. Journal of Autism and Developmental Disorders, 33(3), 239-251. http://dx.doi.org/10.1023/A:1024446215446.
Mervis, C. B., \& Klein-Taxman, B. P. (2004). Methodological issues in group-matching designs: Alpha levels for control variable comparisons and measurement characteristics of control and target variables. Journal of Autism and Developmental Disorders, 34(1), 7-17. http://dx.doi.org/10.1023/B: JADD.0000018069.69562.b8.
Nelson, T. O. (1984). A comparison of current measures of the accuracy of feeling-of-knowing predictions. Psychological Bulletin, 95(1), 109-133. http://dx. doi.org/10.1037//0033-2909.95.1.109.
Nelson, T. O., \& Narens, L. (1990). Metamemory: A theoretical framework and new findings. The Psychology of Learning and Motivation, 26, 125-174. http:// dx.doi.org/10.1016/S0079-7421(08)60053-5.
Nelson, T. O., Narens, L., \& Dunlosky, J. (2004). A revised methodology for research on metamemory: Pre-judgment recall and monitoring (PRAM). Psychological Methods, 9(1), 53-69. http://dx.doi.org/10.1037/1082-989X.9.1.53.
Nichols, S., \& Stich, S. (2003). Mindreading: An integrated account of pretence, self-awareness, and understanding other minds.Oxford: Oxford University Press.
Ozonoff, S., Pennington, B. F., \& Rogers, S. J. (1991). Executive function deficits in high-functioning autistic individuals: Relationship to theory of mind. Journal of Child Psychology and Psychiatry, 32(7), 1081-1105. http://dx.doi.org/10.1111/j.1469-7610.1991.tb00351.x.
Perner, J. (1991). Understanding the representational mind.Cambridge, MA: MIT Press.
Perner, J. (2000). Memory and theory of mind. In E. Tulving \& F. I. M. Craik (Eds.), The oxford handbook of memory (pp. 297-312). Oxford: Oxford University Press.
Roebers, C. M., Schmid, C., \& Roderer, T. (2009). Metacognitive monitoring and control processes involved in primary school children's test performance. British Journal of Educational Psychology, 79(Pt 4), 749-767. http://dx.doi.org/10.1348/978185409X429842.
Russell, J., Hill, E. L., \& Franco, F. (2001). The role of belief veracity in understanding intentions-in-action: Preschool children's performance on the transparent intentions task. Cognitive Development, 16(3), 775-792. http://dx.doi.org/10.1016/S0885-2014(01)00057-0.
Sawyer, A. P., Williamson, P., \& Young, R. (2014). Metacognitive processes in emotion recognition: Are they different in adults with Asperger's disorder? Journal of Autism and Developmental Disorders, 1-10.
Schneider, W., Visé, M., Lockl, K., \& Nelson, T. O. (2000). Developmental trends in children's memory monitoring: Evidence from a judgment-of-learning task. Cognitive Development, 15(2), 115-134. http://dx.doi.org/10.1016/S0885-2014(00)00024-1.
Sitko, B. M. (1998). Knowing how to write: Metacognition and writing instruction. In D. J. Hacker, J. Dunlosky, \& A. Graesser (Eds.), Metacognition in educational theory and practice (pp. 93-115). London: Lawrence Erlbaum Associates.

Terrett, G., Rendell, P. G., Raponi-Saunders, S., Henry, J. D., Bailey, P. E., \& Altgassen, M. (2013). Episodic future thinking in children with autism spectrum disorder. Journal of Autism and Developmental Disorders, 43(11), 2558-2568. http://dx.doi.org/10.1007/s10803-013-1806-y.
Thiede, K. W., Anderson, M. C. M., \& Theriault, D. (2003). Accuracy of metacognitive monitoring affects learning of texts. Journal of Educational Psychology, 95 (1), 66-73. http://dx.doi.org/10.1037/0022-0663.95.1.66.

Wechsler, D. (1999). Wechsler abbreviated scale of intelligence.New York, NY: The Psychological Corporation: Harcourt Brace \& Company.
Weil, L., Fleming, S., Dumontheil, I., Kilford, E., Weil, R., Rees, G., Dolan, R., \& Blakemore, S. (2013). The development of metacognitive ability in adolescence. Consciousness and Cognition, 22(1), 264-271.
Wheeler, M., Stuss, D., \& Tulving, E. (1997). Toward a theory of episodic memory: The frontal lobes and autonoetic consciousness. Psychological Bulletin, 121 (3), $331-354$.

Wilkinson, D. A., Best, C. A., Winshew, N. J., \& Strauss, M. S. (2010). Memory awareness for faces in individuals with autism. Journal of Autism and Developmental Disorders, 40(11), 1371-1377. http://dx.doi.org/10.1007/s10803-010-0995-x.
Williams, D. M. (2010). Theory of own mind in autism. Autism, 14(5), 474-494. http://dx.doi.org/10.1177/1362361310366314.
Williams, D. L., Goldstein, G., \& Winshew, N. J. (2005). Impaired memory for faces and social scenes in autism: Clinical implications of memory dysfunction. Archives of Clinical Neuropsychology, 20(1), 1-15. http://dx.doi.org/10.1016/j.acn.2002.08.001.
Williams, D. M., \& Happé, F. (2009). What did I say? versus What did I think? Attributing false beliefs to self amongst children with and without autism. Journal of Autism and Developmental Disorders, 39(6), 865-873. http://dx.doi.org/10.1007/s10803-009-0695-6.
Wojcik, D. Z., Allen, R. J., Brown, C., \& Souchay, C. (2011). Memory for actions in autism spectrum disorder. Memory, 19(6), 549-558. http://dx.doi.org/ 10.1080/09658211.2011.590506.

World Heath Organisation (1993). International classification of mental and behavioural disorders: Clinical descriptions and diagnostic guidelines (10th ed.) Geneva, Switzerland: World Heath Organisation.
Yeung, N., \& Summerfield, C. (2012). Metacognition in human decision-making: Confidence and error monitoring. Philosophical Transactions of the Royal Society of London. Series B, Biological sciences, 10(1594), 1310-1321.
Yirmiya, N., Erel, O., Shaked, M., \& Solomonica-Levi, D. (1998). Meta-analyses comparing theory of mind abilities of individuals with autism, individuals with mental retardation, and normally developing individuals. Psychological Bulletin, 124(3), 283-307. http://dx.doi.org/10.1037//0033-2909.124.3.283.